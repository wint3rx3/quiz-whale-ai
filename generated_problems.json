[
    {
        "id": 1,
        "title": "Not given",
        "problem": "What is the main feature of the Transformer model proposed in the mentioned research?",
        "choices": {
            "A": "It is based solely on recurrent neural networks",
            "B": "It is based solely on attention mechanisms",
            "C": "It only includes an encoder",
            "D": "It only includes a decoder",
            "E": "It involves convolutional neural networks"
        },
        "answer": "B"
    },
    {
        "id": 2,
        "title": "Not given",
        "problem": "What is the key characteristic of the Transformer model?",
        "choices": {
            "A": "It relies entirely on sequence-aligned RNNs.",
            "B": "It relies entirely on convolution.",
            "C": "It relies entirely on a recurrent network.",
            "D": "It relies entirely on an attention mechanism.",
            "E": "It relies entirely on conditional computation."
        },
        "answer": "D"
    },
    {
        "id": 3,
        "title": "Transformer-model architecture",
        "problem": "What is the output dimension of the sub-layers and embedding layers in the Transformer-model architecture?",
        "choices": {
            "A": "512",
            "B": "1024",
            "C": "256",
            "D": "128",
            "E": "64"
        },
        "answer": "A"
    },
    {
        "id": 4,
        "title": "Scaled Dot-Product and Multi-Head Attention",
        "problem": "Which attention function is more efficient in terms of speed and space?",
        "choices": {
            "A": "Scaled Dot-Product Attention",
            "B": "Multi-Head Attention",
            "C": "Additive Attention",
            "D": "None of the above",
            "E": "Both A and B"
        },
        "answer": "A"
    },
    {
        "id": 5,
        "title": "Not given",
        "problem": "Which of the following is NOT a way the Transformer model uses multi-head attention?",
        "choices": {
            "A": "In encoder-decoder attention layers, where the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.",
            "B": "In self-attention layers, where all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder.",
            "C": "In self-attention layers in the decoder, where each position in the decoder can attend to all positions in the decoder up to and including that position.",
            "D": "In preventing leftward information flow in the decoder to preserve the auto-regressive property.",
            "E": "In analyzing the input sequence in reverse to adjust the model's predictions."
        },
        "answer": "E"
    },
    {
        "id": 6,
        "title": "Not given",
        "problem": "Which of the following steps is not true for the layer types according to the table?",
        "choices": {
            "A": "Self-Attention layer connects all positions with a constant number of sequentially executed operations.",
            "B": "Recurrent layer requires O(n) sequential operations.",
            "C": "Convolutional layers are faster than recurrent layers.",
            "D": "The length of the sequences in recurrent layers is smaller than the representation dimensionality.",
            "E": "To improve computational performance for tasks involving long sequences, self-attention could be restricted."
        },
        "answer": "D"
    },
    {
        "id": 7,
        "title": "Not given",
        "problem": "What can be said about the complexity and behavior of convolutional layers according to the text?",
        "choices": {
            "A": "Convolutional layers are generally more expensive than recurrent layers.",
            "B": "A single convolutional layer with kernel width k<n does not connect all pairs of input and output positions.",
            "C": "The complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer.",
            "D": "All of the above.",
            "E": "None of the above."
        },
        "answer": "D"
    },
    {
        "id": 8,
        "title": "Not given",
        "problem": "Which statement is true regarding the Transformer model for English-to-German and English-to-French news translation tests based on the given information?",
        "choices": {
            "A": "The Transformer model performs poorly on the English-to-German translation test.",
            "B": "The Transformer model achieves lower BLEU scores than previous state-of-the-art models.",
            "C": "Transformer (big) model outperforms other single models and ensembles.",
            "D": "The training cost for the Transformer model is higher than other models.",
            "E": "The BLEU score for the Transformer model for English-to-French translation is lower than 40."
        },
        "answer": "C"
    },
    {
        "id": 9,
        "title": "Not given",
        "problem": "What was observed about the Transformer architecture in Table 3?",
        "choices": {
            "A": "Reducing the attention key size reduces model quality",
            "B": "Bigger models perform better",
            "C": "Dropout is useful in preventing overfitting",
            "D": "Positional encoding with sinusoids and learned positional embeddings produced similar results",
            "E": "All of the above"
        },
        "answer": "E"
    },
    {
        "id": 10,
        "title": "Not given",
        "problem": "Which of the following articles is about 'Neural machine translation by jointly learning to align and translate'?",
        "choices": {
            "A": "Alex Graves. Generating sequences with recurrent neural networks.",
            "B": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate.",
            "C": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.",
            "D": "Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies.",
            "E": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization."
        },
        "answer": "B"
    },
    {
        "id": 11,
        "title": "Understanding neural machine translation approaches",
        "problem": "Which of the following describes the correct contribution of the respective researchers in the field of neural machine translation?",
        "choices": {
            "A": "Oﬁr Press and Lior Wolf used a decomposable attention model.",
            "B": "Minh-Thang Luong, Hieu Pham, and Christopher D Manning developed a system that bridges the gap between human and machine translation.",
            "C": "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean proposed the use of the output embedding to improve language models.",
            "D": "Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit worked on neural machine translation of rare words with subword units.",
            "E": "Rico Sennrich, Barry Haddow, and Alexandra Birch focused on effective approaches to attention-based neural machine translation."
        },
        "answer": "E"
    }
]